# PiLCS
逻辑时间服务（Pi Logical clock Server）是一个高可用、高性能「逻辑时间」服务，可以生成唯一 ID、消息递增版本号、分布式事务 ID 的单调递增序列服务。

## 为什么要做这个系统？

### ID / 序列号的意义

不管在单机系统还是分布式系统里都对 ID / 序列号有着很重的依赖，尤其在复杂的分布式系统中。往往需要对大量的数据和消息进行唯一标识。在数据分库分表后需要有一个唯一ID来标识一条数据，在消息中需要通过 ID 来实现幂等处理，在事务中需要版本号判断先后顺序来处理事务请求。

而 ID / 序列号按照特点又可以分为「唯一 ID 」、「单调递增序列」和 「趋势递增序列」三种：

- 唯一 ID：只要不重复即可。生成的返回方式有 UUID、GUID、等包括单点递增和趋势递增序列都能满足唯一性的要求。单纯的满足唯一 ID 要求的 ID 只能做一些身份标示。
- 趋势递增序列：要求ID 的生成大体上有增长的趋势，比如在一些 B Tree 的数据存储上能保障写入性能。趋势递增序列的算法可以根据时间戳，然后在加上其他逻辑来生成，比如 Twitter 开源的Snowflake 算法、MongoDB 的 Object ID 等思路。
- 单调递增序列：要求下一次生成 ID 一定比上一次的大。用于事务版本号、排序等场景下。通常有数据库自增ID 等。

由上至下，每一级都能满足上一级的特征。单调递增序列完全可以当作趋势递增序列和唯一 ID使用，趋势递增序列又可以当作唯一 ID 使用。但是反过来却不行。

### 常见方法介绍

**UUID、GUID：**标准型式包含32个16进制数字 `123e4567-e89b-12d3-a456-426655440000`。

优点：

- 本地生成、无网络请求、性能高。

缺点：

- 通常用字符串形式使用，有些场景下不适用。
- 不适合做B Tree 存储结构的主键，比如 MySQL 的 DB 引擎。

**Snowflake 类算法：**这类算法大致上把 64 bit 划分为不同的空间。分别用来标示时间、机器编号、自增序列等。利用时间的递增特性、且因为时间戳在高位、所以整体是保持递增趋势的。

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/39cb9821-d379-47e1-b931-3b2d5cafbe84/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/39cb9821-d379-47e1-b931-3b2d5cafbe84/Untitled.png)

优点：

- 性能高，可以本地部署、不依赖网络。
- 可以根据业务特性分配空间。

缺点：

- 强依赖机器时钟，如果时钟回拨可能会重号或者不可用。
- 虽然趋势上是递增的，在分布式系统中不同机器之间的时间并不一定是一致的，无法判断先后顺序。

**数据库自增字段：**就是利用数据库自增字段的特征实现。

优点：

- 简单、稳定，维护成本低。
- 序列单调自增甚至可以是连续递增的。

缺点：

- 强依赖 DB，且是单点系统、即便有主从复制，也有可能会重号。
- 性能低，受限于数据库的读写新能。在 MySQL 开启 Group Replication 的时候，虽然能避免单点问题，但是性能会更低。
- 在分库分表系统中，采用步长的方式来避免 ID 重复，又会失去单调递增的特性。

**[美团的 Leaf 方案](https://tech.meituan.com/2017/04/21/mt-leaf.html)：**也是基于 MySQL 的，不过采用 Leaf-segment 的方式减少对 MySQL 性能压力，比如每个 segment 有 1000 个ID，起点是 0 。Leaf 服务只需要在数据库设置一次 max = 1000，标记从 0 到 1000 的序列已经被分配，当服务重启的时候就从1000 作为起点，不会出现 0 - 1000 之间的序列重复。数据库返回成功之后 Leaf 服务就可以从 0 到 1000 递增分发序列号，1000 次分发序列号只需要一次数据库请求。且不会重复。并且可以同时有多个 Leaf 服务实例同时、MySQL 可以使用 Group Replication 的方式运行避免单点。

优点：

- 可以很方便的线性扩展，性能完全能够支撑大多数业务场景。
- ID号码是趋势递增的8byte的64位数字，满足上述数据库存储的主键要求。
- 容灾性高：Leaf服务内部有号段缓存，即使DB宕机，短时间内Leaf仍能正常对外提供服务。
- 可以自定义max_id的大小，非常方便业务从原有的ID方式上迁移过来。

缺点：

- TP999数据波动大，当号段使用完之后还是会hang在更新数据库的I/O上，tg999数据会出现偶尔的尖刺。
- DB宕机会造成整个系统不可用。
- 整套服务涉及 Leaf 服务、数据库、数据库同步等运维复杂。
- 因为用多个 Leaf 服务实例保障可用性，造成是趋势递增，不是单调递增序列。

**[微信的 seqsvr](https://mp.weixin.qq.com/s?__biz=MzI4NDMyNTU2Mw==&mid=2247483679&idx=1&sn=584dbd80aa08fa1188627ad725680928&mpshare=1&scene=1&srcid=1208L9z4yXKLW60rPph2ZmMn#rd)：**大致思路上与 Leaf 类似，都是基于数据库、分段获取但是在可用性保障上略有不同。

1. 把存储层和缓存中间层分成两个模块StoreSvr及AllocSvr。StoreSvr为存储层，利用了多机NRW策略来保证数据持久化后不丢失；AllocSvr 层负责分发序列号
2. AllocSvr 需要跟 StoreSvr 保持租约。
    1. AllocSvr N 秒内无法从 StoreSvr 读取加载配置时，AllocSvr停止服务。
    2. AllocSvr读取到新的加载配置后，立即卸载需要卸载的号段，需要加载的新号段等待N秒后提供服务。

为了保障单调递增的特性采用单台 AllocSvr 分发序列的形式，AllocSvr 一主一备，当主 AllocSvr 失效后备用服务接管。

优点：

- 可以方便线性扩展，足够支撑业务需求。
- 容灾性高，DB 和单台 AllocSvr 失效在短时间内也能正常提供服务。
- ID 是单调递增，适用于大部分场景。
- 可以根据自身的业务做调整，适配。

缺点：

- 整套服务涉及 AllocSvr、StoreSvr、仲裁服务、数据库、数据库同步等运维复杂。
- AllocSvr、StoreSvr、仲裁服务之间可能会有可用性问题。（主观臆断，没看到具体实现）

总结一下：

1. 生成的序列最好是单调递增的。
2. 集群要高可用。
3. 用批量的方式提高性能。
4. 方便运维。

## 设计思路

### 多台还是单台？

单调递增的序列如果是多台同时提供服务，那么这两台机器之间就需要时时同步以保障不会重复或者倒退。即便依赖纳秒时间戳来生成，也会存在时间漂移问题。如果是单台可以保障发号的序列是单调递增的。

### 性能

通过批量获取的方式来优化单台的性能。每次持久化分配的最大值。这样几乎都是在内存操作没有IO 处理，性能足够满足大部分场景使用。即便单台性能遇到瓶颈，可以用多组实例来分摊不同的业务压力。

### 可用性

可用性要求本身具备容错和 failover 能力。通过多个实例容错的时候，还需要做好与客户端的切换与同步的能力。可用性来说基于 Raft 这类服务就有不错的可用性。

通过以上的分析，PiLCS 的设计方案出来了：

基于 Raft 算法，Leader 对外提供发号服务，利用 Raft 的机制保障高可用。Leader 当选的从持久化数据里获取当前已经分配的最大序列好「X」，然后在这 X 的基础上增加批量获取的大小「N」把 「X + N」作为结果持久化存储。当序列快分发完的时候，异步chi jiu

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/20b9ce79-6cd0-48e0-b1f0-6c986af01a87/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/20b9ce79-6cd0-48e0-b1f0-6c986af01a87/Untitled.png)

选举、寻找 leader 等流程完全使用 Raft 算法。
